{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[],"dockerImageVersionId":30887,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"from matplotlib import pyplot as plt\nfrom huggingface_hub import hf_hub_download\nfrom datasets import load_dataset\nfrom sklearn.metrics import f1_score\nimport pandas as pd\nimport optuna\n\nimport torch\nfrom torch import nn\nfrom torch.optim import Adam\nfrom torch.optim.lr_scheduler import ReduceLROnPlateau\nfrom torch.utils.data import DataLoader\nfrom torchvision import transforms\nfrom torchvision.models import resnet152, ResNet152_Weights","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-03-12T07:20:57.081093Z","iopub.execute_input":"2025-03-12T07:20:57.081466Z","iopub.status.idle":"2025-03-12T07:20:57.086804Z","shell.execute_reply.started":"2025-03-12T07:20:57.081437Z","shell.execute_reply":"2025-03-12T07:20:57.085559Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"ds = load_dataset(\n    \"mikkoim/aquamonitor-jyu\",\n    cache_dir=\"/kaggle/working/\"\n)\n\nhf_hub_download(\n    repo_id=\"mikkoim/aquamonitor-jyu\",\n    filename=\"aquamonitor-jyu.parquet.gzip\",\n    repo_type=\"dataset\",\n    local_dir=\"/kaggle/working/\"\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-12T07:20:59.602379Z","iopub.execute_input":"2025-03-12T07:20:59.602773Z","iopub.status.idle":"2025-03-12T07:21:00.794062Z","shell.execute_reply.started":"2025-03-12T07:20:59.602743Z","shell.execute_reply":"2025-03-12T07:21:00.793014Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# dataset elements can be accessed with indices. Each \"row\" or record\n# has an image and a key that can be used to access data from the metadata table\nrecord = ds[\"train\"][21015]\nprint(record, \"\\n\")\n\nimg = record[\"jpg\"]\nprint(record[\"__key__\"])\nimg","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-12T07:21:09.917167Z","iopub.execute_input":"2025-03-12T07:21:09.9175Z","iopub.status.idle":"2025-03-12T07:21:09.933046Z","shell.execute_reply.started":"2025-03-12T07:21:09.917473Z","shell.execute_reply":"2025-03-12T07:21:09.931933Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# The keys match the rows in the metadata table\nmetadata = pd.read_parquet(\"/kaggle/working/aquamonitor-jyu.parquet.gzip\")\nmetadata","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-12T07:21:12.16617Z","iopub.execute_input":"2025-03-12T07:21:12.166692Z","iopub.status.idle":"2025-03-12T07:21:12.371007Z","shell.execute_reply.started":"2025-03-12T07:21:12.166643Z","shell.execute_reply":"2025-03-12T07:21:12.369931Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"classes = sorted(metadata[\"taxon_group\"].unique())\nclass_map = {k:v for v,k in enumerate(classes)}\nclass_map_inv = {v:k for k,v in class_map.items()}\n\nmetadata[\"img\"] = metadata[\"img\"].str.removesuffix(\".jpg\")\nlabel_dict = dict(zip(metadata[\"img\"], metadata[\"taxon_group\"].map(class_map)))\n\nclass_map_inv","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-12T07:21:15.977508Z","iopub.execute_input":"2025-03-12T07:21:15.977876Z","iopub.status.idle":"2025-03-12T07:21:16.02415Z","shell.execute_reply.started":"2025-03-12T07:21:15.977847Z","shell.execute_reply":"2025-03-12T07:21:16.023039Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"IMAGE_SIZE = 224\nBATCH_SIZE = 32\nEPOCH_NUM = 7","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-12T07:21:18.609207Z","iopub.execute_input":"2025-03-12T07:21:18.609561Z","iopub.status.idle":"2025-03-12T07:21:18.613735Z","shell.execute_reply.started":"2025-03-12T07:21:18.609534Z","shell.execute_reply":"2025-03-12T07:21:18.612644Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"tf = transforms.Compose([\n    transforms.Resize((IMAGE_SIZE, IMAGE_SIZE)),\n    # transforms.RandomHorizontalFlip(p=0.75),\n    # transforms.RandomVerticalFlip(p=0.75),\n    transforms.ToTensor(),\n    transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5])\n])\n\ndef preprocess(batch):\n    return {\"key\": batch[\"__key__\"],\n            \"img\": [tf(x) for x in batch[\"jpg\"]],\n            \"label\": torch.as_tensor([label_dict[x] for x in batch[\"__key__\"]], dtype=torch.long)}\n\ntrain_ds = ds[\"train\"].with_transform(preprocess)\ndevel_ds = ds[\"validation\"].with_transform(preprocess)\n\nprint(f\"Train Size: {train_ds.num_rows}\")\nprint(f\"Devel Size: {devel_ds.num_rows}\")\n\n# plt.imshow(train_ds[2014][\"img\"].permute(1, 2, 0).numpy())","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-12T07:21:47.491341Z","iopub.execute_input":"2025-03-12T07:21:47.491844Z","iopub.status.idle":"2025-03-12T07:21:48.000734Z","shell.execute_reply.started":"2025-03-12T07:21:47.491804Z","shell.execute_reply":"2025-03-12T07:21:47.999715Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"train_loader = DataLoader(\n    train_ds,\n    batch_size=BATCH_SIZE,\n    shuffle=True\n)\n\ndevel_loader = DataLoader(\n    devel_ds,\n    batch_size=BATCH_SIZE\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-12T07:21:49.828673Z","iopub.execute_input":"2025-03-12T07:21:49.829009Z","iopub.status.idle":"2025-03-12T07:21:49.836474Z","shell.execute_reply.started":"2025-03-12T07:21:49.828982Z","shell.execute_reply":"2025-03-12T07:21:49.835084Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"model = resnet152(weights=ResNet152_Weights.DEFAULT)\n\nmodel.fc = nn.Linear(\n    in_features=model.fc.in_features,\n    out_features=len(classes)\n)\n\ncriterion = nn.CrossEntropyLoss(label_smoothing=0.1)\n\noptimizer = torch.optim.Adam(model.parameters(), lr=1e-4)\n\nscheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n    optimizer,\n    mode=\"min\",\n    factor=0.5,\n    patience=5\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-12T07:29:19.517337Z","iopub.execute_input":"2025-03-12T07:29:19.517757Z","iopub.status.idle":"2025-03-12T07:29:20.918225Z","shell.execute_reply.started":"2025-03-12T07:29:19.517722Z","shell.execute_reply":"2025-03-12T07:29:20.917341Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nmodel.to(device)\n\n# Initialise objects to store results\ntrain_losses = []\ntrain_f1s = []\ndevel_losses = []\ndevel_f1s = []\nbest_loss = float(\"inf\")\nbest_model_weights = None\n\nfor epoch in range(EPOCH_NUM):\n\n    model.train()\n    running_loss = 0.0\n    train_labels = []\n    train_preds = []\n    \n    for batch in train_loader:\n        images, labels = batch[\"img\"], batch[\"label\"]\n        images, labels = images.to(device), labels.to(device)\n        \n        optimizer.zero_grad()\n        outputs = model(images)\n        loss = criterion(outputs, labels)\n        loss.backward()\n        optimizer.step()\n        \n        running_loss += loss.item()\n        \n        # Store predictions\n        _, preds = torch.max(outputs.data, 1)\n        train_labels.extend(labels.cpu().numpy())\n        train_preds.extend(preds.cpu().numpy())\n    \n    # Calculate and store training loss and f1 score\n    train_loss = running_loss / len(train_loader)\n    train_f1 = f1_score(train_labels, train_preds, average=\"weighted\")\n    train_losses.append(train_loss)\n    train_f1s.append(train_f1)\n\n    # Validation phase\n    model.eval()\n    devel_running_loss = 0.0\n    devel_labels = []\n    devel_preds = []\n    \n    with torch.no_grad():\n        for batch in devel_loader:\n            images, labels = batch[\"img\"], batch[\"label\"]\n            images, labels = images.to(device), labels.to(device)\n            \n            outputs = model(images)\n            loss = criterion(outputs, labels)\n            devel_running_loss += loss.item()\n            \n            # Store predictions\n            _, preds = torch.max(outputs.data, 1)\n            devel_labels.extend(labels.cpu().numpy())\n            devel_preds.extend(preds.cpu().numpy())\n    \n    # Calculate and store validation loss and f1 score\n    devel_loss = devel_running_loss / len(devel_loader)\n    devel_f1 = f1_score(devel_labels, devel_preds, average=\"weighted\")\n    devel_losses.append(devel_loss)\n    devel_f1s.append(devel_f1)\n\n    # Step the scheduler\n    scheduler.step(devel_loss)\n\n    print(f\"Epoch [{epoch+1}/{EPOCH_NUM}], lr: {scheduler.get_last_lr()[0]}, \"\n          f\"Train Loss: {train_loss:.4f}, Train F1-Score: {train_f1:.3g}, \"\n          f\"Devel Loss: {devel_loss:.4f}, Devel F1-Score: {devel_f1:.3g}\")\n\n    # Check if we have a new best model\n    if devel_loss < best_loss:\n        best_loss = devel_loss\n        best_model_weights = model.state_dict()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-27T19:45:08.034062Z","iopub.execute_input":"2025-02-27T19:45:08.034505Z","iopub.status.idle":"2025-02-27T19:45:08.938984Z","shell.execute_reply.started":"2025-02-27T19:45:08.034469Z","shell.execute_reply":"2025-02-27T19:45:08.937524Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Restore best weights\nif best_model_weights is not None:\n\n    # Load best model\n    model.load_state_dict(best_model_weights)\n    # Save best model\n    torch.save(best_model_weights, \"/kaggle/working/fine_tuned_resnet18.pth\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Plot the learning curves\nplt.figure(figsize=(12, 5))\n\n# Loss plot\nplt.subplot(1, 2, 1)\nplt.plot(range(1, EPOCH_NUM + 1), train_losses, label=\"Train Set\")\nplt.plot(range(1, EPOCH_NUM + 1), devel_losses, label=\"Devel Set\")\nplt.title(\"Loss Curve\")\nplt.xlabel('Epochs')\nplt.ylabel('Loss')\nplt.legend()\n\n# F1-Score plot\nplt.subplot(1, 2, 2)\nplt.plot(range(1, EPOCH_NUM + 1), train_f1s, label=\"Train Set\")\nplt.plot(range(1, EPOCH_NUM + 1), devel_f1s, label=\"Devel Set\")\nplt.title(\"F1-Score Curve\")\nplt.xlabel(\"Epochs\")\nplt.ylabel(\"F1-Score\")\nplt.legend()\n\nplt.tight_layout()","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}